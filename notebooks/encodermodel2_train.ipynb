{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b473eb36",
   "metadata": {},
   "source": [
    "## This is the training file for the local model which stands as a proof of concept currently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8a3fae2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m     12\u001b[39m sys.path.append(\u001b[33m\"\u001b[39m\u001b[33m~/Desktop/projects/audio-filler\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioEncoder1\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"~/Desktop/projects/audio-filler\")\n",
    "from models.model2.model2 import AudioEncoder1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e66df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicGenreDataset(Dataset):\n",
    "    def __init__(self, data_dir, clip_duration=15, stride=5, sample_rate=16000):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.clip_length = clip_duration * sample_rate\n",
    "        self.stride = stride * sample_rate\n",
    "        self.sample_rate = sample_rate\n",
    "        \n",
    "        # Get genre labels\n",
    "        self.genres = sorted([d.name for d in self.data_dir.iterdir() if d.is_dir()])\n",
    "        self.genre_to_idx = {genre: idx for idx, genre in enumerate(self.genres)}\n",
    "        \n",
    "        # Collect audio files and their genres\n",
    "        self.audio_files = []\n",
    "        for genre in self.genres:\n",
    "            genre_dir = self.data_dir / genre\n",
    "            for audio_file in genre_dir.glob(\"*.mp3\"):\n",
    "                self.audio_files.append((audio_file, genre))\n",
    "        \n",
    "        # Precompute clip segments\n",
    "        self.clips = []\n",
    "        for audio_file, genre in self.audio_files:\n",
    "            info = torchaudio.info(audio_file)\n",
    "            total_samples = info.num_frames\n",
    "            start = 0\n",
    "            while start < total_samples:\n",
    "                end = start + self.clip_length\n",
    "                if end > total_samples:\n",
    "                    if total_samples - start >= 10 * sample_rate:  # Check if at least 10s\n",
    "                        end = total_samples\n",
    "                        padding = self.clip_length - (end - start)\n",
    "                        self.clips.append((audio_file, start, end, padding, genre))\n",
    "                    break\n",
    "                else:\n",
    "                    self.clips.append((audio_file, start, end, 0, genre))\n",
    "                start += self.stride\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.clips)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_file, start, end, padding, genre = self.clips[idx]\n",
    "        waveform, sr = torchaudio.load(audio_file, frame_offset=start, num_frames=end-start)\n",
    "        waveform = torchaudio.functional.resample(waveform, sr, self.sample_rate)[0]  # Convert to mono\n",
    "        \n",
    "        if padding > 0:\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
    "        \n",
    "        # Add channel dimension\n",
    "        waveform = waveform.unsqueeze(0)\n",
    "        label = self.genre_to_idx[genre]\n",
    "        \n",
    "        return waveform, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376b6587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset and dataloader\n",
    "dataset = MusicGenreDataset(\"path/to/your/data\")\n",
    "dataloader = DataLoader(dataset, batch_size=1000, shuffle=True, num_workers=4)\n",
    "\n",
    "# Model and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AudioModel1().to(device)\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Track metrics\n",
    "metrics = {\n",
    "    'total_loss': [], 'classification_loss': [], \n",
    "    'modulus_recon_loss': [], 'sign_recon_loss': [],\n",
    "    'sign_accuracy': [], 'kl_loss': []\n",
    "}\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for batch_idx, (data, targets) in enumerate(dataloader):\n",
    "    data, targets = data.to(device), targets.to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    losses = model.loss_function(data, targets)\n",
    "    losses['total_loss'].backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Store metrics\n",
    "    for k in metrics:\n",
    "        metrics[k].append(losses[k].item())\n",
    "    \n",
    "    # Print batch statistics\n",
    "    print(f\"Batch {batch_idx+1}:\")\n",
    "    print(f\"Total Loss: {losses['total_loss'].item():.4f}\")\n",
    "    print(f\"Classification Loss: {losses['classification_loss'].item():.4f}\")\n",
    "    print(f\"Modulus Recon Loss: {losses['modulus_recon_loss'].item():.4f}\")\n",
    "    print(f\"Sign Recon Loss: {losses['sign_recon_loss'].item():.4f}\")\n",
    "    print(f\"Sign Accuracy: {losses['sign_accuracy'].item():.4f}\")\n",
    "    print(f\"KL Loss: {losses['kl_loss'].item():.4f}\")\n",
    "    \n",
    "    # Plot every 5 batches\n",
    "    if (batch_idx + 1) % 5 == 0:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        for i, (k, v) in enumerate(metrics.items()):\n",
    "            plt.subplot(2, 3, i+1)\n",
    "            plt.plot(v, label=k)\n",
    "            plt.title(k)\n",
    "            plt.xlabel('Batch')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio-filler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
